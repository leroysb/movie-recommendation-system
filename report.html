<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>BUILDING A MOVIE RECOMMENDATION SYSTEM USING THE MOVIELENS RESEARCH DATA</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>BUILDING A MOVIE RECOMMENDATION SYSTEM USING THE MOVIELENS RESEARCH DATA</h1>

<h2>1. INTRODUCTION</h2>

<p>In this project, we create a movie recommendation system using the MovieLens dataset. You can obtain the GroupLens Research data from <a href="http://files.grouplens.org/datasets/movielens/ml-10m.zip">this link</a>.  Our main aim is to perform a data analysis and through visualization, find patterns to assist us in building a model that will provide an optimum movie recommendation to users.  </p>

<p>The dataset is composed of 10000054 observations with:</p>

<ul>
<li>69878 unique users and</li>
<li>10677 movies</li>
</ul>

<p>The key steps performed include:</p>

<ul>
<li>Preparation of the work environment.</li>
<li>Preparation, exploration and visualizations of the data</li>
<li>Analysis of the obsertions.</li>
<li>Calculation of the optimal RMSE based on movieId and userId.</li>
</ul>

<p>On following the above steps, our data model will reveal that the best predictors used to provide the optimum recommendation system are moveiId and UserId. The RMSE is <strong>0.8431355</strong>.</p>

<h2>2. METHODS AND ANALYSIS</h2>

<h3>2.1 Work Environment Preparation</h3>

<p>We are going to use the following libraries:</p>

<pre><code class="r">if(!require(tidyverse)) install.packages(&quot;tidyverse&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
if(!require(caret)) install.packages(&quot;caret&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
if(!require(data.table)) install.packages(&quot;data.table&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
if(!require(knitr)) install.packages(&quot;knitr&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
if(!require(rmarkdown)) install.packages(&quot;rmarkdown&quot;, repos = &quot;http://cran.us.r-project.org&quot;)
</code></pre>

<p>Next we shall download the data</p>

<pre><code class="r">dl &lt;- tempfile()
download.file(&quot;http://files.grouplens.org/datasets/movielens/ml-10m.zip&quot;, dl)
</code></pre>

<h3>2.2 Data Wrangling</h3>

<p>We build the desired dataset which we shall use to build our model using the code below:  </p>

<pre><code class="r">ratings &lt;- fread(text = gsub(&quot;::&quot;, &quot;\t&quot;, readLines(unzip(dl, &quot;ml-10M100K/ratings.dat&quot;))),
                 col.names = c(&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;, &quot;timestamp&quot;))
</code></pre>

<pre><code>## Error: memory exhausted (limit reached?)
</code></pre>

<pre><code class="r">movies &lt;- str_split_fixed(readLines(unzip(dl, &quot;ml-10M100K/movies.dat&quot;)), &quot;\\::&quot;, 3)
colnames(movies) &lt;- c(&quot;movieId&quot;, &quot;title&quot;, &quot;genres&quot;)
movies &lt;- as.data.frame(movies) %&gt;% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens &lt;- left_join(ratings, movies, by = &quot;movieId&quot;)
</code></pre>

<pre><code>## Error: cannot allocate vector of size 76.3 Mb
</code></pre>

<pre><code class="r"># Validation set will be 10% of MovieLens data
set.seed(1)
test_index &lt;- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
</code></pre>

<pre><code>## Error: cannot allocate vector of size 76.3 Mb
</code></pre>

<pre><code class="r">edx &lt;- movielens[-test_index,]
</code></pre>

<pre><code>## Error: cannot allocate vector of size 38.1 Mb
</code></pre>

<pre><code class="r">temp &lt;- movielens[test_index,]

validation &lt;- temp %&gt;% semi_join(edx, by = &quot;movieId&quot;) %&gt;% semi_join(edx, by = &quot;userId&quot;)

removed &lt;- anti_join(temp, validation)
edx &lt;- rbind(edx, removed)
</code></pre>

<pre><code>## Error: cannot allocate vector of size 68.7 Mb
</code></pre>

<pre><code class="r">#Add a year column generated from the timestamp column
dates &lt;- as.Date(as.POSIXct(edx$timestamp, origin=&quot;1970-01-01&quot;))
</code></pre>

<pre><code>## Error: cannot allocate vector of size 68.7 Mb
</code></pre>

<pre><code class="r">edx &lt;- edx %&gt;% mutate(year=year(dates), month=month(dates))
</code></pre>

<pre><code>## Error: cannot allocate vector of size 34.3 Mb
</code></pre>

<p>We shall use the edx dataset we just created onwards to train our model.</p>

<h3>2.3 Data Exploration and Visualizations</h3>

<p>The edx dataset consists of:<br>
Total number of ratings  </p>

<pre><code class="r">length(edx$rating)
</code></pre>

<pre><code>## [1] 9000055
</code></pre>

<p>Total number of movies  </p>

<pre><code class="r">n_distinct(edx$movieId)
</code></pre>

<pre><code>## [1] 10677
</code></pre>

<p>Total number of users  </p>

<pre><code class="r">n_distinct(edx$userId)
</code></pre>

<pre><code>## [1] 69878
</code></pre>

<p>And we can confirm that each user rated a movie using the following code  </p>

<pre><code class="r">edx %&gt;% filter(is.na(.$ratings))
</code></pre>

<p>The top 20 most viewed genres are  </p>

<pre><code>##                              genres  count
## 1                             Drama 733296
## 2                            Comedy 700889
## 3                    Comedy|Romance 365468
## 4                      Comedy|Drama 323637
## 5              Comedy|Drama|Romance 261425
## 6                     Drama|Romance 259355
## 7           Action|Adventure|Sci-Fi 219938
## 8         Action|Adventure|Thriller 149091
## 9                    Drama|Thriller 145373
## 10                      Crime|Drama 137387
## 11                        Drama|War 111029
## 12             Crime|Drama|Thriller 106101
## 13 Action|Adventure|Sci-Fi|Thriller 105144
## 14            Action|Crime|Thriller 102259
## 15                 Action|Drama|War  99183
## 16                  Action|Thriller  96535
## 17           Action|Sci-Fi|Thriller  95280
## 18                         Thriller  94662
## 19                  Horror|Thriller  75000
## 20                     Comedy|Crime  73286
</code></pre>

<p>While the top 20 best rated movies are  </p>

<pre><code>## Error: &lt;text&gt;:1:140: unexpected symbol
## 1: edx %&gt;% group_by(movieId) %&gt;% summarize(title = title[1], count = n()) %&gt;% top_n(20, count) %&gt;%  arrange(desc(count)) %&gt;% select(-movieId) as.data.frame
##                                                                                                                                                ^
</code></pre>

<p>From the figure below, we can conclude that <strong>1997</strong> had the highest number of ratings  </p>

<pre><code>## Error: Column `year` must be length 1 (a summary value), not 0
</code></pre>

<p>And the genre <strong>Drama/War</strong> with the highest average ratings.<br>
<img src="figure/unnamed-chunk-11-1.png" alt="plot of chunk unnamed-chunk-11"></p>

<h3>2.4 Data Analysis and Modelling</h3>

<p>Let&#39;s look at some of the general properties of the data to better understand the challenge.</p>

<p>The first observation is that some users are more active than others at rating movies.
Notice that some users have rated over 1,000 movies while others have only rated a handful.<br>
<img src="figure/unnamed-chunk-12-1.png" alt="plot of chunk unnamed-chunk-12"></p>

<p>The second thing we notice is that some movies get rated more than others. Here is the distribution.<br>
<img src="figure/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13"></p>

<p>From the two observations above, we can then prove that there&#39;s indeed a movie variability and a user variability. We will use these two predictors to model the data. </p>

<p>To compare different models or to see how well we&#39;re doing compared to some baseline, we need to quantify what it means to do well. We need a loss function, in this case the residual mean squared error since we can interpret it as similar to standard deviation. It is the typical error we make when predicting a movie rating. This will therefore be our modelling approach. </p>

<p>We&#39;re going to predict the same rating for all movies, regardless of the user and movie. In this case, that&#39;s just the average of all the ratings. Using that, we&#39;ll get the first RMSE.</p>

<pre><code class="r">RMSE &lt;- function(true_ratings, predicted_ratings)
  {sqrt(mean((true_ratings - predicted_ratings)^2))}

mu_hat &lt;- mean(train_set$rating)
naive_rmse &lt;- RMSE(test_set$rating, mu_hat)
naive_rmse
</code></pre>

<pre><code>## [1] 1.059904
</code></pre>

<p>As we go along we will be comparing different approaches, we&#39;re going to create a table that&#39;s going to store the results that we obtain as we go along. </p>

<pre><code class="r">rmse_results &lt;- data_frame(method = &quot;Just the average&quot;, RMSE = naive_rmse)
</code></pre>

<h3>2.5 LAMBDA and RMSE Calculations</h3>

<p>Now let&#39;s see how much our prediction improves once we predict using the model that we just fit.</p>

<pre><code class="r">mu &lt;- mean(train_set$rating)

movie_avgs &lt;- train_set %&gt;% group_by(movieId) %&gt;% summarize(b_i = mean(rating - mu))

movie_avgs %&gt;% qplot(b_i, geom =&quot;histogram&quot;, bins = 10, data = ., color = I(&quot;black&quot;))
</code></pre>

<p><img src="figure/unnamed-chunk-16-1.png" alt="plot of chunk unnamed-chunk-16"></p>

<pre><code class="r">predicted_ratings &lt;- mu + test_set %&gt;% left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;% .$b_i

model_1_rmse &lt;- RMSE(predicted_ratings, test_set$rating)

rmse_results &lt;- bind_rows(rmse_results, data_frame(method=&quot;Movie Effect Model&quot;, RMSE = model_1_rmse ))
</code></pre>

<p>Our residual mean squared error did drop a little bit. From <strong>1.0599043</strong> to <strong>0.9437429</strong></p>

<table><thead>
<tr>
<th align="left">method</th>
<th align="right">RMSE</th>
</tr>
</thead><tbody>
<tr>
<td align="left">Just the average</td>
<td align="right">1.0599043</td>
</tr>
<tr>
<td align="left">Movie Effect Model</td>
<td align="right">0.9437429</td>
</tr>
</tbody></table>

<p>We continue to make it better. This time we factor in user variability. Are different users different in terms of how they rate movies? To explore the data, let&#39;s compute the average rating for user, u, for those that have rated over 100 movies.</p>

<p><img src="figure/unnamed-chunk-18-1.png" alt="plot of chunk unnamed-chunk-18"></p>

<p>Note that there is substantial variability across users, as well. Some users are very cranky. And others love every movie they watch, while others are somewhere in the middle. Now we move ahead and implement user variability.</p>

<pre><code class="r">user_avgs &lt;- test_set %&gt;% left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;% group_by(userId) %&gt;%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings &lt;- test_set %&gt;% left_join(movie_avgs, by=&#39;movieId&#39;) %&gt;%
  left_join(user_avgs, by=&#39;userId&#39;) %&gt;% mutate(pred = mu + b_i + b_u) %&gt;% .$pred

model_2_rmse &lt;- RMSE(predicted_ratings, test_set$rating)

rmse_results &lt;- bind_rows(rmse_results,data_frame
                          (method=&quot;Movie + User Effects Model&quot;,  RMSE = model_2_rmse ))
</code></pre>

<h2>3. RESULT</h2>

<p>We see that now we obtain a further improvement. Our residual mean squared error dropped down to about <strong>0.84</strong>, hence this is our optimal model. </p>

<pre><code class="r">rmse_results %&gt;% knitr::kable()
</code></pre>

<table><thead>
<tr>
<th align="left">method</th>
<th align="right">RMSE</th>
</tr>
</thead><tbody>
<tr>
<td align="left">Just the average</td>
<td align="right">1.0599043</td>
</tr>
<tr>
<td align="left">Movie Effect Model</td>
<td align="right">0.9437429</td>
</tr>
<tr>
<td align="left">Movie + User Effects Model</td>
<td align="right">0.8431355</td>
</tr>
</tbody></table>

<h2>4. CONCLUSION</h2>

<p>The main objective of the project was to predict movie ratings from a long list of rated movies where we achieved an optimal RMSE of <strong>0.8431355</strong>. Throughout the analysis, some important trends were discovered, some of which were used in modelling the data like user and movie variability. Can we make the model even more better? A challenge worthy of research. </p>

</body>

</html>
